---
layout: post
title: '⌈YOLO series⌋ —— YOLOv1'
date: 2024-08-03
author: 洪茬铭
cover: 'https://pic.imgdb.cn/item/66addcedd9c307b7e978176c.jpg'
tags: 目标检测
---

> [论文地址](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)  |  相关图片内容取自原文及b站 [霹雳吧啦Wz大佬的讲解视频](https://www.bilibili.com/video/BV1yi4y1g7ro/?spm_id_from=333.999.0.0&vd_source=d2c93f88f96dd7f6158ea87a409c1583)
>
> **本文目录：**
>
> 1. 论文思想
> 2. 网络结构
> 3. 损失函数
> 4. 模型缺陷



# 1. 论文思想

![](https://pic.imgdb.cn/item/66ade3b5d9c307b7e97f3ac3.jpg)

**YOLOv1** 将一幅图像中所有类别的目标检测统一到了一个端到端的框架之中。在这个框架中，只需要执行三步：

- 将输入图像的大小调整为 `448 × 448`

- 在图像上运行一个卷积网络

- 根据模型的置信度对检测结果进行阈值化处理

![](https://pic.imgdb.cn/item/66ade32dd9c307b7e97e254c.jpg)

首先，将输入的图像分成 `S×S` 个网格 (grid cell)，如果某个 object 的中心落在这个网格中，则这个网格就负责预测这个 object。每个网格需要预测 `B` 个 bounding box，每个 bounding box 除了要预测位置之外，还要附带预测一个 confidence 值，除此之外，每个网格还要预测 `C` 个类别的分数。在论文中，使用了 PASCAL VOC 数据集，共 20 类目标，设置 `S=7, B=2, C=20`，输出维度为 `7×7×30`。YOLOv1 没有 anchor 的概念，所有的关于位置的信息都是通过卷积直接生成的。

**输出的结构：**

![](https://pic.imgdb.cn/item/66adf1a4d9c307b7e994b5fd.jpg)

对于每个 bounding box，有 5 个参数，分别是 `x, y, w, h, confidence`。其中，`(x, y)`  坐标表示**相对**于网格单元边界的框的中心，范围在 0 到1 之间。`(w, h)` 是**相对**于整个图像预测的，范围也是在 0 到 1 之间。最后，`confidence` 分数反映了模型对该方框包含一个物体的置信度，以及模型认为其预测的方框的准确度，形式上定义为：

$$
\mathrm{confidence} = \mathrm{Pr}(\mathrm{Object})*\mathrm{IOU}_{\mathrm{pred}}^{\mathrm{truth}} \tag1
$$

其中，$\mathrm{Pr}(\mathrm{Object})$ 相当于一个二值函数，如果当前网格 (cell) 中没有需要检测的 object，那么 $\mathrm{Pr}(\mathrm{Object})=0$ ，置信度分数为零。否则，我们希望置信度分数等于预测框与 GT 之间的交集。**也就是说，`confidence` 反映的是当前预测的这个 bounding box 内包含物体的概率以及其预测的方框的准确度，具体要想判断是哪一类 object，需要结合后面的 20 类分数。**

每个网格单元还能预测 `C` 个**条件类别概率 (conditional class probabilities)**，这些概率以包含对象的网格单元为条件，形式上定义为：

$$
\Pr(\text{Class}i|\text{Object}) \tag2
$$

每个网格只预测一组类概率，与方格 `B` 的数量无关。**测试时**，将条件类别概率和单个方框置信度预测值相乘：

$$
\mathrm{Pr}(\mathrm{Class}_i|\mathrm{Object})*\mathrm{Pr}(\mathrm{Object})*\mathrm{IOU}_{\mathrm{pred}}^{\mathrm{truth}}=\mathrm{Pr}(\mathrm{Class}_i)*\mathrm{IOU}_{\mathrm{pred}}^{\mathrm{truth}} \tag3
$$

从而得出每个方框中特定类别的置信度分数。这些分数**既表示该类出现在方框中的概率，也表示预测方框与对象的匹配程度**。

# 2. 网络结构

![](https://pic.imgdb.cn/item/66adf83ad9c307b7e9a22716.jpg)

网络结构如上图所示，只是包含了 24 个卷积层和 2 个全连接层。

# 3. 损失函数

![](https://pic.imgdb.cn/item/66ae0637d9c307b7e9b69e24.jpg)

整个损失函数包含了三个方面的损失，主要使用了误差平方和 (sum-squared error) 的形式：

- **bounding box 损失**

- **confidence 损失**

- **classes 损失**

其中，$\mathbb{1}_i^{\mathrm{obj}}$  表示物体是否出现在单元格 i 中，$\mathbb{1}_{ij}^{\mathrm{obj}}$ 表示单元格 i 中的第 j 个边框预测器对该预测 "负责"。请注意，损失函数只在对象出现在该网格单元时才会对分类错误进行惩罚（因此才有了前面讨论的条件类概率）。如果预测因子对地面实况框 "负责"（即在该网格单元的所有预测因子中具有最高的 IOU），它也只对边界框坐标错误进行惩罚。

至于为什么在计算宽高的损失时使用了根号差，这是为了消除目标尺寸对偏移误差带来的影响。偏移相同的距离，对小尺寸目标的检测带来的影响更大，所以我们需要扩大在相同偏移量下小目标的损失来提高对小目标的检测精度。

![](https://pic.imgdb.cn/item/66ae07aad9c307b7e9b826fd.jpg)



# 4. 模型缺陷

- YOLOv1 对边框预测施加了很强的空间限制，因为每个网格单元只能预测两个边框，并且只能有一个类别。这种空间约束限制了我们的模型所能预测的附近物体的数量。我们的模型**很难预测成群出现的小物体**，例如鸟群。*(因为此时有许多相同目标的中心落在了同一个 grid cell 里，而一个 grid cell 最多预测两个目标)*
- 由于我们的模型是从数据中学习预测边界框的，因此它**很难泛化到新的或不寻常的长宽比或配置的物体上**。我们的模型还使用了相对粗糙的特征来预测边界框，因为我们的架构对输入图像进行了多层降采样。
- 最后，虽然我们使用近似检测性能的损失函数进行训练，但我们的损失函数对小检测框和大检测框中的错误一视同仁。大检测框中的小误差通常是无害的，但小检测框中的小误差对 IOU 的影响要大得多。**我们的主要错误来源是不正确的定位。** *(主要是因为直接预测目标框参数，而不是基于 anchor)*



















